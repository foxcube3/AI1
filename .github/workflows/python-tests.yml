name: Python Tests

on:
  push:
    branches: [ "**" ]
  pull_request:
    branches: [ "**" ]

jobs:
  test:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: [ "3.9", "3.10", "3.11" ]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies (if any)
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          python -m pip install ruff flake8 build

      - name: Lint (ruff)
        run: |
          ruff --version
          ruff check .

      - name: Lint (flake8)
        run: |
          flake8 . --max-line-length 160 --extend-ignore E501,W292,E702,E741,F401,F841,E203,W503

      - name: Run unit tests
        run: |
          python -m unittest discover -s tests -p "test_*.py" -v

      - name: Smoke test train_and_embed pipeline
        run: |
          python examples/train_and_embed.py --corpus allen.txt --vocab_size 200 --min_frequency 2 --output_prefix ci_bpe --dim 8 --text "Allen allows ample analysis"

      - name: Smoke test learned positional embedding persistence
        run: |
          python examples/example_learned_pe_persist.py --merges ci_bpe_merges.txt --vocab ci_bpe_vocab.json --text "Allen allows ample analysis" --dim 8 --max_len 64 --pe_out ci_learned_pe.json

      - name: Smoke test transformer encoder example
        run: |
          python examples/example_transformer_encoder.py --merges ci_bpe_merges.txt --vocab ci_bpe_vocab.json --text "Allen allows ample analysis" --dim 16 --layers 2 --heads 4 --ff 32 --add_pe

      - name: Smoke test next-token head training
        run: |
          python examples/train_next_token_head.py --corpus allen.txt --merges ci_bpe_merges.txt --vocab ci_bpe_vocab.json --dim 16 --layers 1 --heads 2 --ff 32 --seq_len 8 --stride 8 --epochs 1 --lr 0.02 --save_head ci_head.json

      - name: Smoke test next-token inference
        run: |
          python examples/infer_next_token.py --text "Allen allows" --head ci_head.json --merges ci_bpe_merges.txt --vocab ci_bpe_vocab.json --dim 16 --layers 1 --heads 2 --ff 32 --top_k 5

      - name: Smoke test train-then-chatbot (single turn via stdin)
        run: |
          echo "Hello from CI" | python examples/train_then_chatbot.py --corpus allen.txt --merges ci_bpe_merges.txt --vocab ci_bpe_vocab.json --dim 16 --layers 1 --heads 2 --ff 32 --seq_len 8 --stride 8 --epochs 1 --lr 0.02 --add_pe --save_head ci_head_chat.json --max_new_tokens 8 --temperature 0.9 --top_k 5

      - name: Upload trained head artifact
        uses: actions/upload-artifact@v4
        with:
          name: trained-head-${{ matrix.python-version }}
          path: |
            ci_head.json
            ci_head_chat.json

      - name: Benchmark grid (quick)
        run: |
          python examples/benchmark_transformer_grid.py --seq_grid "8,16" --dim 16 --heads 4 --layers 1 --ff 32 --repeats 1

      - name: Build distributions
        run: |
          python -m build

      - name: Upload dist artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-${{ matrix.python-version }}
          path: dist/*

  publish:
    name: Publish to PyPI (on tag)
    needs: test
    runs-on: ubuntu-latest
    # Use env. context instead of secrets. in the if-expression to avoid context restrictions.
    if: startsWith(github.ref, 'refs/tags/') && env.PYPI_API_TOKEN != ''
    env:
      PYPI_API_TOKEN: ${{ secrets.PYPI_API_TOKEN }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install build + twine
        run: |
          python -m pip install --upgrade pip
          python -m pip install build twine

      - name: Build
        run: |
          python -m build

      - name: Publish to PyPI
        env:
          TWINE_USERNAME: __token__
          TWINE_PASSWORD: ${{ env.PYPI_API_TOKEN }}
        run: |
          twine upload dist/*